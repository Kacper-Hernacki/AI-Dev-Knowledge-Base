/**
 * Before/After Model Hooks Example
 * Demonstrates using hooks to process state before and after model calls
 */

import { ChatAnthropic } from "@langchain/anthropic";
import {
  MemorySaver,
  MessagesAnnotation,
  StateGraph,
  Annotation,
} from "@langchain/langgraph";
import { RemoveMessage, trimMessages } from "@langchain/core/messages";

/**
 * Demonstrate before model hooks
 */
export async function demonstrateBeforeModelHooks() {
  console.log("\n‚¨ÖÔ∏è  Before Model Hooks");
  console.log("=".repeat(50));

  const checkpointer = new MemorySaver();
  const model = new ChatAnthropic({
    model: "claude-3-5-haiku-20241022",
    temperature: 0,
  });

  // Create workflow with before-model processing
  const workflow = new StateGraph(MessagesAnnotation)
    .addNode("beforeModel", async (state) => {
      console.log(`  ‚öôÔ∏è  Before model: Processing ${state.messages.length} messages`);

      // Trim messages before model call
      const trimmed = await trimMessages(state.messages, {
        maxTokens: 200,
        strategy: "last",
        tokenCounter: (msgs) => msgs.length,
        startOn: "human",
        endOn: ["human", "tool"],
      });

      console.log(`  ‚úÇÔ∏è  Trimmed to ${trimmed.length} messages`);

      return { messages: trimmed };
    })
    .addNode("agent", async (state) => {
      const response = await model.invoke(state.messages);
      return { messages: [response] };
    })
    .addEdge("__start__", "beforeModel")
    .addEdge("beforeModel", "agent")
    .addEdge("agent", "__end__");

  const agent = workflow.compile({ checkpointer });

  try {
    console.log("\n1Ô∏è‚É£ Building conversation:");

    const turns = [
      "Message 1: Hello!",
      "Message 2: How are you?",
      "Message 3: Tell me about AI",
      "Message 4: What's machine learning?",
      "Message 5: Summarize our conversation",
    ];

    for (const content of turns) {
      console.log(`\n   User: ${content}`);
      await agent.invoke(
        { messages: [{ role: "user", content }] },
        { configurable: { thread_id: "before_hook_1" } }
      );
    }

    console.log("\n‚úÖ Before hooks process messages before model!");
  } catch (error) {
    console.error("‚ùå Before model hooks demo failed:", error);
  }
}

/**
 * Demonstrate after model hooks
 */
export async function demonstrateAfterModelHooks() {
  console.log("\n‚û°Ô∏è  After Model Hooks");
  console.log("=".repeat(50));

  const RequestCountAnnotation = Annotation.Root({
    ...MessagesAnnotation.spec,
    requestCount: Annotation<number>({
      reducer: (prev, curr) => (curr !== undefined ? curr : prev),
      default: () => 0,
    }),
  });

  const checkpointer = new MemorySaver();
  const model = new ChatAnthropic({
    model: "claude-3-5-haiku-20241022",
    temperature: 0,
  });

  // Create workflow with after-model processing
  const workflow = new StateGraph(RequestCountAnnotation)
    .addNode("agent", async (state) => {
      const response = await model.invoke(state.messages);
      return { messages: [response] };
    })
    .addNode("afterModel", (state) => {
      console.log(`  ‚öôÔ∏è  After model: Processing response`);

      // Increment request counter
      const newCount = state.requestCount + 1;
      console.log(`  üìä Request count: ${newCount}`);

      // Validate response
      const lastMessage = state.messages[state.messages.length - 1];
      const content =
        typeof lastMessage.content === "string"
          ? lastMessage.content
          : "";

      // Filter sensitive content
      if (content.toLowerCase().includes("confidential")) {
        console.log("  üö´ Removing sensitive content");
        return {
          requestCount: newCount,
          messages: [new RemoveMessage({ id: lastMessage.id! })],
        };
      }

      return { requestCount: newCount };
    })
    .addEdge("__start__", "agent")
    .addEdge("agent", "afterModel")
    .addEdge("afterModel", "__end__");

  const agent = workflow.compile({ checkpointer });

  try {
    console.log("\n1Ô∏è‚É£ Normal message:");
    const response1 = await agent.invoke(
      {
        messages: [{ role: "user", content: "Hello!" }],
      },
      { configurable: { thread_id: "after_hook_1" } }
    );
    console.log(`  Request count: ${response1.requestCount}`);

    console.log("\n2Ô∏è‚É£ Another message:");
    const response2 = await agent.invoke(
      {
        messages: [{ role: "user", content: "How are you?" }],
      },
      { configurable: { thread_id: "after_hook_1" } }
    );
    console.log(`  Request count: ${response2.requestCount}`);

    console.log("\n‚úÖ After hooks process responses after model!");
  } catch (error) {
    console.error("‚ùå After model hooks demo failed:", error);
  }
}

/**
 * Demonstrate combined before/after hooks
 */
export async function demonstrateCombinedHooks() {
  console.log("\nüîÑ Combined Before/After Hooks");
  console.log("=".repeat(50));

  const MetricsAnnotation = Annotation.Root({
    ...MessagesAnnotation.spec,
    inputTokens: Annotation<number>({
      reducer: (prev, curr) => prev + (curr || 0),
      default: () => 0,
    }),
    outputTokens: Annotation<number>({
      reducer: (prev, curr) => prev + (curr || 0),
      default: () => 0,
    }),
  });

  const checkpointer = new MemorySaver();
  const model = new ChatAnthropic({
    model: "claude-3-5-haiku-20241022",
    temperature: 0,
  });

  const workflow = new StateGraph(MetricsAnnotation)
    .addNode("beforeModel", (state) => {
      // Estimate input tokens
      const inputSize = state.messages.reduce((sum, msg) => {
        const content =
          typeof msg.content === "string" ? msg.content : "";
        return sum + content.length;
      }, 0);

      console.log(`  üì• Before: ~${inputSize} input chars`);

      return { inputTokens: inputSize };
    })
    .addNode("agent", async (state) => {
      const response = await model.invoke(state.messages);
      return { messages: [response] };
    })
    .addNode("afterModel", (state) => {
      // Estimate output tokens
      const lastMessage = state.messages[state.messages.length - 1];
      const outputSize =
        typeof lastMessage.content === "string"
          ? lastMessage.content.length
          : 0;

      console.log(`  üì§ After: ~${outputSize} output chars`);
      console.log(`  üìä Total: ~${state.inputTokens + outputSize} chars`);

      return { outputTokens: outputSize };
    })
    .addEdge("__start__", "beforeModel")
    .addEdge("beforeModel", "agent")
    .addEdge("agent", "afterModel")
    .addEdge("afterModel", "__end__");

  const agent = workflow.compile({ checkpointer });

  try {
    console.log("\n1Ô∏è‚É£ Tracking metrics:");

    await agent.invoke(
      {
        messages: [
          { role: "user", content: "Tell me about TypeScript" },
        ],
      },
      { configurable: { thread_id: "combined_1" } }
    );

    const response = await agent.invoke(
      {
        messages: [
          { role: "user", content: "What are its benefits?" },
        ],
      },
      { configurable: { thread_id: "combined_1" } }
    );

    console.log(`\n  üìä Total input: ${response.inputTokens}`);
    console.log(`  üìä Total output: ${response.outputTokens}`);

    console.log("\n‚úÖ Combined hooks provide full request lifecycle!");
  } catch (error) {
    console.error("‚ùå Combined hooks demo failed:", error);
  }
}

/**
 * Demonstrate hook patterns and use cases
 */
export function demonstrateHookPatterns() {
  console.log("\nüìã Hook Patterns and Use Cases");
  console.log("=".repeat(50));

  console.log("\n‚¨ÖÔ∏è  Before Model Hooks:");
  console.log("  ‚Ä¢ Trim/filter messages");
  console.log("  ‚Ä¢ Validate input");
  console.log("  ‚Ä¢ Add context from state");
  console.log("  ‚Ä¢ Inject system prompts");
  console.log("  ‚Ä¢ Log requests");

  console.log("\n‚û°Ô∏è  After Model Hooks:");
  console.log("  ‚Ä¢ Filter/validate responses");
  console.log("  ‚Ä¢ Update metrics");
  console.log("  ‚Ä¢ Remove sensitive content");
  console.log("  ‚Ä¢ Log responses");
  console.log("  ‚Ä¢ Trigger side effects");

  console.log("\nüîÑ Combined Patterns:");
  console.log("  ‚Ä¢ Request/response logging");
  console.log("  ‚Ä¢ Token usage tracking");
  console.log("  ‚Ä¢ Content moderation pipeline");
  console.log("  ‚Ä¢ Performance monitoring");
  console.log("  ‚Ä¢ Error handling");

  console.log("\n‚öôÔ∏è  Implementation Tips:");
  console.log("  ‚Ä¢ Keep hooks focused and single-purpose");
  console.log("  ‚Ä¢ Use state annotations for hook data");
  console.log("  ‚Ä¢ Handle errors gracefully");
  console.log("  ‚Ä¢ Test hooks independently");
}

// Run if executed directly
if (import.meta.main) {
  await demonstrateBeforeModelHooks();
  await demonstrateAfterModelHooks();
  await demonstrateCombinedHooks();
  demonstrateHookPatterns();
}
